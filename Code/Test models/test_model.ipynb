{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "from snntorch import spikeplot as splt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# importing sound classification models from torchaudio\n",
    "import torchaudio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:\n",
      "1\n",
      "List of devices:\n",
      "<torch.cuda.device object at 0x0000020B5766B490>\n",
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# List the available devices:\n",
    "print(\"Available devices:\")\n",
    "print(torch.cuda.device_count())\n",
    "print(\"List of devices:\")\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "#print(torch.cuda.device(1))\n",
    "#print(torch.cuda.get_device_name(1))\n",
    "device_ids = [0, 1]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "## Structure of the dataset is as follows:\n",
    "### dataset_folder -> Folder_name_of_one_class -> audio_file.wav\n",
    "dataset_folder = '/kaggle/input/audioset/TrainSet'\n",
    "animals_folder = dataset_folder + '/Animals'+'/animals_segmented'\n",
    "music_folder = dataset_folder + '/Music'+'/instruments_segmented'\n",
    "sot_folder = dataset_folder + '/SoT'+'/sound_of_things_segmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Le chemin d’accès spécifié est introuvable: '/kaggle/input/audioset/TrainSet/Animals/animals_segmented'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Putting the data paths into dictionaries (key: class, value: list of file names)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m data_paths \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manimals\u001b[39m\u001b[38;5;124m'\u001b[39m: [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(animals_folder, file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43manimals_folder\u001b[49m\u001b[43m)\u001b[49m],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmusic\u001b[39m\u001b[38;5;124m'\u001b[39m: [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(music_folder, file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(music_folder)],\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msot\u001b[39m\u001b[38;5;124m'\u001b[39m: [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sot_folder, file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(sot_folder)]\n\u001b[0;32m      6\u001b[0m }\n\u001b[0;32m      8\u001b[0m animals_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manimals\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusic\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msot\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     12\u001b[0m }\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# print len of each class\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Le chemin d’accès spécifié est introuvable: '/kaggle/input/audioset/TrainSet/Animals/animals_segmented'"
     ]
    }
   ],
   "source": [
    "# Putting the data paths into dictionaries (key: class, value: list of file names)\n",
    "data_paths = {\n",
    "    'animals': [os.path.join(animals_folder, file) for file in os.listdir(animals_folder)],\n",
    "    'music': [os.path.join(music_folder, file) for file in os.listdir(music_folder)],\n",
    "    'sot': [os.path.join(sot_folder, file) for file in os.listdir(sot_folder)]\n",
    "}\n",
    "\n",
    "animals_dict = {\n",
    "    \"animals\": 0,\n",
    "    \"music\": 1,\n",
    "    \"sot\": 2\n",
    "}\n",
    "\n",
    "# print len of each class\n",
    "print(len(data_paths['animals']))\n",
    "print(len(data_paths['music']))\n",
    "print(len(data_paths['sot']))\n",
    "print(\"Total number of files: \", len(data_paths['animals']) + len(data_paths['music']) + len(data_paths['sot']))\n",
    "\n",
    "print(data_paths['animals'][:5])\n",
    "\n",
    "# Tuple to tensor of numbers\n",
    "def name_tuple_to_float_tensor(tuple):\n",
    "    # Read the content of the tupe and use animals_dict to convert the class name to a number in a new tensor\n",
    "    return torch.tensor([animals_dict[tuple[i]] for i in range(len(tuple))], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "num_classes = 3\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_paths, transform=None):\n",
    "        self.data_paths = data_paths\n",
    "        self.classes = list(data_paths.keys())\n",
    "        self.files = sum([data_paths[cls] for cls in self.classes], [])\n",
    "        self.transform = transform\n",
    "        self.printPath = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        for cls in self.classes:\n",
    "            if idx < len(self.data_paths[cls]):\n",
    "                file = self.data_paths[cls][idx]\n",
    "                classe = cls\n",
    "                break\n",
    "            idx -= len(self.data_paths[cls])\n",
    "        else:\n",
    "            raise IndexError('Index out of range')\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(file)\n",
    "        if self.transform:\n",
    "            mfcc = self.transform(waveform)\n",
    "        \n",
    "        if True:\n",
    "            # Taking a mean tensor of the mfcc 2 channel tensor\n",
    "            mfcc = torch.mean(mfcc, dim=0).detach() \n",
    "    \n",
    "        if self.printPath:\n",
    "            return mfcc, sample_rate, classe, file\n",
    "        else:\n",
    "            return mfcc, sample_rate, classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, RandomApply\n",
    "\n",
    "transforms = torchaudio.transforms.MFCC(\n",
    "    sample_rate= 48000,\n",
    "    n_mfcc=20)\n",
    "\n",
    "transforms_2 = Compose([\n",
    "    RandomApply([torchaudio.transforms.PitchShift(sample_rate=48000, n_steps=2)], p=0.4),\n",
    "    RandomApply([torchaudio.transforms.FrequencyMasking(freq_mask_param=15)], p=0.2),  # SpecAugment\n",
    "    RandomApply([torchaudio.transforms.TimeMasking(time_mask_param=35)], p=0.2),  # SpecAugment\n",
    "    torchaudio.transforms.MFCC(sample_rate=48000, n_mfcc=20),\n",
    "    #torchaudio.transforms.SlidingWindowCmn(cmn_window=600, min_cmn_window=100, center=False, norm_vars=False), # https://dsp.stackexchange.com/questions/19564/cepstral-mean-normalization\n",
    "])\n",
    "\n",
    "transform_norm_and_MFFC = Compose([\n",
    "    torchaudio.transforms.SlidingWindowCmn(cmn_window=600, min_cmn_window=100, center=False, norm_vars=False), # https://dsp.stackexchange.com/questions/19564/cepstral-mean-normalization\n",
    "    torchaudio.transforms.MFCC(sample_rate=48000, n_mfcc=20),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = AudioDataset(data_paths, transform=transforms)\n",
    "dataset2 = AudioDataset(data_paths, transform=transforms_2)\n",
    "dataset3 = AudioDataset(data_paths, transform=transform_norm_and_MFFC)\n",
    "mfcctensor, sample_rate, classe = dataset1.__getitem__(0)\n",
    "print(mfcctensor.shape, sample_rate, classe)\n",
    "\n",
    "print(mfcctensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select one channel of the MFCC tensor\n",
    "mfcc_channel_1 = mfcctensor.detach().numpy() # mfcctensor[0].detach().numpy()\n",
    "\n",
    "# Plot the MFCC\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mfcc_channel_1, x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('MFCC')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_size = int(0.8 * len(dataset1)) # + len(dataset2))) #int(0.8 * (len(dataset1) + len(dataset2) + len(dataset3)))\n",
    "val_size = int(0.15 * len(dataset1)) #+ len(dataset2))) #int(0.1 * (len(dataset1) + len(dataset2) + len(dataset3)))\n",
    "test_size = len(dataset1)  - train_size - val_size #  + len(dataset2) - train_size - val_size #len(dataset1) + len(dataset2) + len(dataset3) - train_size - val_size\n",
    "\n",
    "dataset1.printPath = True\n",
    "dataset2.printPath = True\n",
    "dataset3.printPath = True\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset1, [train_size, val_size, test_size])  # + dataset2, [train_size, val_size, test_size]) # torch.utils.data.random_split(dataset1 + dataset2 + dataset3, [train_size, val_size, test_size])\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "print(len(train_dataset) + len(val_dataset) + len(test_dataset))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1, pin_memory=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1, pin_memory=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first element of the dataloader\n",
    "dataiter = iter(train_dataloader)\n",
    "mfcctensor, sample_rate, classe, file = next(dataiter)\n",
    "print(mfcctensor.shape, sample_rate, classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"csnn_simple_model\"\n",
    "# Load the model\n",
    "model = torch.load(\"../Models/Trained on raw data/\" + model_name + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on 1 audio file\n",
    "model.eval()\n",
    "mfcctensor = mfcctensor.cuda()\n",
    "output = model(mfcctensor)\n",
    "print(output)\n",
    "print(torch.argmax(output, dim=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
